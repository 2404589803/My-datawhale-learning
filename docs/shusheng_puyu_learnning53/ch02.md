# 第2节 轻松分钟玩转书生·浦语大模型趣味 Demo

## 一、使用的模型及机器

本次使用的模型为：**InternLM-Chat-7B`模型**
机器配置：**浦语Intern云服务器NOVIDA 1/4 A100**

## 二、如何一个对话demo(形式：编辑器demo、网页demo)

### 1、编辑器demo

#### ①新建一个conda环境

因为第一个环境出现问题，因此我用以下代码创建了另一个conda环境。

指令为：

```
conda create --name internlm-demo2 --clone=/root/share/conda_envs/internlm-base
```
#### ② 激活环境

指令为：

```
conda activate internlm-demo2
```

#### ③ 安装依赖

指令为：

```
python -m pip install --upgrade pip

pip install modelscope==1.9.5

pip install transformers==4.35.2

pip install streamlit==1.24.0

pip install sentencepiece==0.1.99

pip install accelerate==0.24.1
```

#### ④ 模型下载

InternStudio平台的 share 目录下已经为我们准备了全系列的 InternLM 模型，所以我们可以直接复制即可。使用如下命令复制：

```
mkdir -p /root/model/Shanghai_AI_Laboratory

cp -r /root/share/temp/model_repos/internlm-chat-7b /root/model/Shanghai_AI_Laboratory
```

#### ⑤ clone 代码

首先在 /root 路径下新建 code 目录，然后切换路径, clone 代码.

```
cd /root/code
git clone https://gitee.com/internlm/InternLM.git
```
#### ⑥ 切换commit 版本

切换 commit 版本，与教程 commit 版本保持一致，可以让更好的复现。

```
cd InternLM
git checkout 3028f07cb79e5b1d7342f4ad8d11efad3fd13d17
```
将 /root/code/InternLM/web_demo.py 中 29 行和 33 行的模型更换为本地的 /root/model/Shanghai_AI_Laboratory/internlm-chat-7b

#### ⑦ 编辑器终端运行

我们需要在 /root/code/InternLM 目录下新建一个 cli_demo.py 文件，将以下代码填入其中：

```
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


model_name_or_path = "/root/model/Shanghai_AI_Laboratory/internlm-chat-7b"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')
model = model.eval()

system_prompt = """You are an AI assistant whose name is InternLM (书生·浦语).
- InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.
- InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.
"""

messages = [(system_prompt, '')]

print("=============Welcome to InternLM chatbot, type 'exit' to exit.=============")

while True:
    input_text = input("User  >>> ")
    input_text = input_text.replace(' ', '')
    if input_text == "exit":
        break
    response, history = model.chat(tokenizer, input_text, history=messages)
    messages.append((input_text, response))
    print(f"robot >>> {response}")
```
然后，执行以下代码运行这个文件，就可以与InternLM-Chat-7B 模型对话了：
```
python /root/code/InternLM/cli_demo.py
```
### 2、浏览器demo

#### ①新建一个conda环境

因为第一个环境出现问题，因此我用以下代码创建了另一个conda环境。

指令为：

```
conda create --name internlm-demo2 --clone=/root/share/conda_envs/internlm-base
```
#### ② 激活环境

指令为：

```
conda activate internlm-demo2
```

#### ③ 安装依赖

指令为：

```
python -m pip install --upgrade pip

pip install modelscope==1.9.5

pip install transformers==4.35.2

pip install streamlit==1.24.0

pip install sentencepiece==0.1.99

pip install accelerate==0.24.1
```

#### ④ 模型下载

InternStudio平台的 share 目录下已经为我们准备了全系列的 InternLM 模型，所以我们可以直接复制即可。使用如下命令复制：

```
mkdir -p /root/model/Shanghai_AI_Laboratory

cp -r /root/share/temp/model_repos/internlm-chat-7b /root/model/Shanghai_AI_Laboratory
```

#### ⑤ clone 代码

首先在 /root 路径下新建 code 目录，然后切换路径, clone 代码.

```
cd /root/code
git clone https://gitee.com/internlm/InternLM.git
```
#### ⑥ 切换commit 版本

切换 commit 版本，与教程 commit 版本保持一致，可以让更好的复现。

```
cd InternLM
git checkout 3028f07cb79e5b1d7342f4ad8d11efad3fd13d17
```
将 /root/code/InternLM/web_demo.py 中 29 行和 33 行的模型更换为本地的 /root/model/Shanghai_AI_Laboratory/internlm-chat-7b

#### ⑦将服务器端口映射到本地

注：我已创建过密钥对，且如果不将端口映射到本地，则无法使用浏览器进行访问。

首先我们打开创建的开发机，点击SSH连接，可得知与本地服务器进行连接的字符：

我的字符为：
```
ssh -o StrictHostKeyChecking=no -p xxxx root@ssh.intern-ai.org.cn
```
然后我们复制以下指令到本地Power Shell 终端中：

```
ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p 33090
```
33090这个端口根据开发机ssh的端口更改（就是把33090用xxxx替换），6006这个端口是固定的，不可更改。

#### ⑧ 运行web_demo

本地连接完之后，回到开发机的vscode界面，在vscode的终端界面运行以下代码：

```
conda activate internlm-demo2 #这个指令根据自己创建的虚拟环境来改
cd /root/code/InternLM
streamlit run web_demo.py --server.address 127.0.0.1 --server.port 6006

```
