# 第5节 LMDeploy 的量化和部署

## 一、环境配置

首先,我们可以使用 vgpu-smi 指令查看显卡资源使用情况。

显卡资源使用情况如下：

![Alt text](%E6%98%BE%E5%8D%A1%E8%B5%84%E6%BA%90%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5.PNG)

接下来,我们切换到刚刚的终端，创建部署和量化需要的环境。建议大家使用官方提供的环境，使用 conda 直接复制。

这里 /share/conda_envs 目录下的环境是官方未大家准备好的基础环境，因为该目录是共享只读的，而我们后面需要在此基础上安装新的软件包，所以需要复制到我们自己的 conda 环境（该环境下我们是可写的）。

执行如下命令：

```
$ conda create -n lmdeploy --clone /share/conda_envs/internlm-base
```

如果clone操作过慢，可采用如下操作:

```
$ /root/share/install_conda_env_internlm_base.sh lmdeploy

```

最后激活环境

执行如下命令：

```
$ conda activate lmdeploy
```

完成上述步骤后，我们可以进入Python检查一下 PyTorch 和 lmdeploy 的版本。由于 PyTorch 在官方提供的环境里，我们应该可以看到版本显示，而 lmdeploy 需要我们自己安装，此处应该会提示“没有这个包”，输入下面几行代码进行验证：

```
$ python
>>> import torch
>>> torch.__version__
>>> import lmdeploy

```

报错情况如下：

![Alt text](%E6%8A%A5%E9%94%99%E5%9B%BE.PNG)

如图所示，lmdeploy 没有安装，我们接下来手动安装一下，建议安装最新的稳定版。如果是在 InternStudio 开发环境，需要先运行下面的命令，否则会报错。

```
# 解决 ModuleNotFoundError: No module named 'packaging' 问题
pip install packaging

# 使用 flash_attn 的预编译包解决安装过慢问题
pip install /root/share/wheels/flash_attn-2.4.2+cu118torch2.0cxx11abiTRUE-cp310-cp310-linux_x86_64.whl

pip install 'lmdeploy[all]==v0.1.0'

```
由于默认安装的是 runtime 依赖包，但是我们这里还需要部署和量化，所以，这里选择 [all]。然后可以再检查一下 lmdeploy 包。

## 二、服务部署

该技术的架构图如图所示：

![Alt text](lmdeploy.drawio.png)

我们把从架构上把整个服务流程分成下面几个模块。

- 模型推理/服务。主要提供模型本身的推理，一般来说可以和具体业务解耦，专注模型推理本身性能的优化。可以以模块、API等多种方式提供。

- Client。可以理解为前端，与用户交互的地方。

- API Server。一般作为前端的后端，提供与产品和服务相关的数据和功能支持。

值得说明的是，以上的划分是一个相对完整的模型，但在实际中这并不是绝对的。比如可以把“模型推理”和“API Server”合并，有的甚至是三个流程打包在一起提供服务。

接下来，我们看一下lmdeploy提供的部署功能。

### 2.1 模型转换

使用 TurboMind 推理模型需要先将模型转化为 TurboMind 的格式，目前支持在线转换和离线转换两种形式。在线转换可以直接加载 Huggingface 模型，离线转换需需要先保存模型再加载。

TurboMind 是一款关于 LLM 推理的高效推理引擎，基于英伟达的 FasterTransformer 研发而成。它的主要功能包括：LLaMa 结构模型的支持，persistent batch 推理模式和可扩展的 KV 缓存管理器。

#### 2.1.1 在线转换

lmdeploy 支持直接读取 Huggingface 模型权重，目前共支持三种类型：

在 huggingface.co 上面通过 lmdeploy 量化的模型，如 llama2-70b-4bit, internlm-chat-20b-4bit
huggingface.co 上面其他 LM 模型，如 Qwen/Qwen-7B-Chat

示例如下：

```
# 需要能访问 Huggingface 的网络环境
lmdeploy chat turbomind internlm/internlm-chat-20b-4bit --model-name 
internlm-chat-20b

lmdeploy chat turbomind Qwen/Qwen-7B-Chat --model-name qwen-7b

```

上面两行命令分别展示了如何直接加载 Huggingface 的模型，第一条命令是加载使用 lmdeploy 量化的版本，第二条命令是加载其他 LLM 模型。

我们也可以直接启动本地的 Huggingface 模型，如下所示。

```
lmdeploy chat turbomind /share/temp/model_repos/internlm-chat-7b/  --model-name internlm-chat-7b
```

以上命令都会启动一个本地对话界面，通过 Bash 可以与 LLM 进行对话。

#### 2.1.2 离线转换

离线转换需要在启动服务之前，将模型转为 lmdeploy TurboMind 的格式，步骤如下所示。

```
# 转换模型（FastTransformer格式） TurboMind
lmdeploy convert internlm-chat-7b /path/to/internlm-chat-7b

```

这里我们使用官方提供的模型文件，就在用户根目录执行，如下所示。

```
lmdeploy convert internlm-chat-7b  /root/share/temp/model_repos/internlm-chat-7b/

```
执行完成后将会在当前目录生成一个 workspace 的文件夹。这里面包含的就是 TurboMind 和 Triton “模型推理”需要到的文件。

## 三、作业

### 3.1 作业一：使用 LMDeploy 以本地对话、网页Gradio、API服务中的一种方式部署 InternLM-Chat-7B 模型，生成 300 字的小故事（需截图）

结果如下：

```
(lmdeploy) root@intern-studio:~# lmdeploy chat turbomind /share/temp/model_repos/internlm-chat-7b/  --model-name internlm-chat-7b
model_source: hf_model
WARNING: Can not find tokenizer.json. It may take long time to initialize the tokenizer.
WARNING: Can not find tokenizer.json. It may take long time to initialize the tokenizer.
model_config:
{
  "model_name": "internlm-chat-7b",
  "tensor_para_size": 1,
  "head_num": 32,
  "kv_head_num": 32,
  "vocab_size": 103168,
  "num_layer": 32,
  "inter_size": 11008,
  "norm_eps": 1e-06,
  "attn_bias": 1,
  "start_id": 1,
  "end_id": 2,
  "session_len": 2056,
  "weight_type": "fp16",
  "rotary_embedding": 128,
  "rope_theta": 10000.0,
  "size_per_head": 128,
  "group_size": 0,
  "max_batch_size": 64,
  "max_context_token_num": 1,
  "step_length": 1,
  "cache_max_entry_count": 0.5,
  "cache_block_seq_len": 128,
  "cache_chunk_size": 1,
  "use_context_fmha": 1,
  "quant_policy": 0,
  "max_position_embeddings": 2048,
  "rope_scaling_factor": 0.0,
  "use_logn_attn": 0
}
get 323 model params
[WARNING] gemm_config.in is not found; using default GEMM algo                                                                 
session 1

double enter to end input >>> 你是谁
、

<|System|>:You are an AI assistant whose name is InternLM (书生·浦语).
- InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.
- InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.

<|User|>:你是谁
、
<|Bot|>: 我是

书生·浦语，你的AI助手。有什么我可以帮助你的吗？

double enter to end input >>> 
<|User|>:
<|Bot|>: 是的，我可以帮你完成许多任务，例如回答问题、提供定义和解释、将文本从一种语言翻译成另一种语言、总结文本、生成文本、编写故事、分析情感、提供推荐、开发算法、编写代码以及其他任何基于语言的任务。请告诉我你需要我做什么。

double enter to end input >>> 
<|User|>:
<|Bot|>: 好的，我会尽力帮助你。请问有什么我可以帮助你的吗？

double enter to end input >>> 生成 300 字的小故事


<|User|>:生成 300 字的小故事
<|Bot|>: 好的，以下是我为您生成的小故事：

从前，有一个小男孩叫做小明。他非常喜欢探险，总是梦想着能够发现一些神秘的地方。有一天，他听说了一个传说，说在森林深处有一座神秘的城堡，里面藏着无数的宝藏。小明决定要去探险，寻找这座城堡。

他穿过了茂密的森林，越过了险峻的山脉，终于来到了城堡的门前。城堡的大门紧闭着，但是小明并没有放弃。他开始寻找城堡的入口，最终他发现了一个隐藏的通道。

小明进入了通道，发现里面非常黑暗。他摸索着向前走，终于来到了城堡的中心。他发现这里有一个巨大的宝藏，里面装满了金子、宝石和珍贵的文物。

小明非常兴奋，但是他并没有贪婪。他决定把宝藏分给其他人，让他们也能分享这份财富。于是，他开始收集宝藏，最终把它分成了三份。

小明带着自己的那份宝藏回到了家，他决定把这份财富用来帮助那些需要帮助的人。他成立了一个慈善组织，为贫困的人们提供帮助。

小明的故事告诉我们，即使面对困难和挑战，只要我们勇敢地去探索，就有可能发现美好的事物。同时，我们也应该像小明一样，用我们的财富来帮助那些需要帮助的人。
```

结果图如下：

![Alt text](%E4%BD%9C%E4%B8%9A1.PNG)